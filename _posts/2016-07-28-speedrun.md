---
layout: post
section-type: post
title: The Tableau Drag Race
category: Speed
tags: [ 'benchmark', 'performance' ]
---

This is a blog post that has been in the making for about a year. It started at TC15 where I had a chance meeting with one Anthony Krinsky.

![Krinsky](https://cmtoomey.github.io/img/krinsky.jpg)

If you haven't met him, he is a Senior Sales Consultant at Tableau, and an all around nice guy. He had been sent out to the expo floor to find out as much as he could about the next generation of database technology. Cloud, in-memory, GPU, MPP, all the things. Why?

## Extracts

![extracts](https://cmtoomey.github.io/img/extracts.png)

Yes, Tableau just bought HyPer (or was about to at that time), but these little packets of data are showing their age. If you use them at all, you know they can take forever to build, can chew up massive amounts of memory, consume all your CPU cycles, and if they are on Server they probably fail all the time.

Anecdotally, the upper bound for extract size is 500m records (that's not a hard and fast rule, but that's in the ballpark). At that size, combined with the 2.5-3x memory requirement for creating them, it will eat your machine for lunch. Sure you can offload it through the Extract API, but your still hitting your DB for a long time and pegging your machine while you do it. And then you refresh and pray you don't set your Server on fire.

At that size of data (which really isn't that large), you either have to start aggregating or restricting what data you collect. That defeats the entire purpose of BI and Tableau because you can never see and understand your data correctly and you may miss out on something important.

You are also wasting time and resources because you are creating more work for yourself. Every time you aggregate, you are removing detail that can be useful to explain **WHY** something happened. If you add up all the time you spend going back, re-querying, and re-aggregating the dataset every time you get a question, you'll probably find a very large number.

I get that some aggregation may be necessary, but if you are doing more than one pass just to make extracts run faster or fit in your DB, go buy PowerBI.

> *Side note: I know storage is basically free at this point, and processing on Hadoop is much easier now, but please don't get all FOMO with your data. Thinking something would be "nice to have" or "we should collect all this stuff because it will probably be useful later" is not a thing.*

Here's one more reason to consider avoiding extracts - cash. For those companies that use core-based licensing, and who don't run more than a few extracts, you should realize that the extra 8 cores you just bought to manage all your background jobs is pure profit for Tableau.

Server, while frustrating at times, is very very good at managing users, querying data, and rendering your dashboards. A reasonably sized server with 8 cores, decent memory, and a solid CPU can handle hundreds of concurrent users. You wouldn't need more cores, except for those extract jobs.

Think about that the next time you renew your Server license.

---

## Ok, Extracts are old and expensive, so what?

Anthony and I believe that we've reached a technological point where you shouldn't have to create extracts, ever. The combination of GPUs, cloud technologies, and better processing techniques have led us to a place where we should be able to query all our data live, all the time.

### Live data, all the time

Think about that for a minute. How it would simplify your life. Server Admin is now drastically simpler, you never have to manage TWBXs, you can use real version control on TWB files, you can give more people more access to data, and you can focus all your energy on delivering data and insight. Oh, and save money.

That's a world I want to live in, and a world Tableau wants to live in.

Are we there yet?

---

# The Drag Race

The only way we could think of to answer this question is to test it. Real data at real-world scale. Not some synthetic benchmark, it had to be real data at the scale of business.

We selected a list of modern database systems that could potentially be a replacement for the extract. These were both traditional SQL systems as well as Hadoop-backed setups.

Our requirements are simple:

+ The DB has to be able to provide query response times in the 30s range. This is the upper bound for user abandonment.
+ The data has to be large, no smaller than 5 billion records.
+ Dashboards have to be usable, and are also subject to the 30s rule.
+ It can't break the bank, it has to be accessible to regular companies.

Where would we find data this size? Event Streams.

## Background

At Zillow (my current employer), we generate a lot of data. I can't tell you exactly how much, because it's always changing. But it's a lot, and we use Google Analytics to capture it. Unlike other companies, that traffic is a core metric for us, so we need to be able to count correctly at scale. Our sister company, Trulia, uses a similar system, Omniture (now an Adobe product).

We are in the midst of evaluating the future of BI and Business Analytics at Zillow, which is the source of the results you will be seeing below. I'll be talking more about that process later (but not on this blog).

## The Benchmark

How are we going to test? I designed a test based around the standard Tableau workflow.

+ Count things (and distinct things)
+ See the Trend
+ Filter some data
+ Top N with metrics
+ Build a Dashboard
+ Add filters and actions
+ Send lots of users to it via TabJolt
+ Build a more complicated dashboard that doesn't adhere to "[Efficient Design](https://blog.databender.net/2016/06/16/best-practices-for-designing-efficient-tableau-workbooks-the-v10-0-edition/)"

If your DB can't do those things, it can't be used for more complicated things.

I also devised what I like to call a "Hammer Query." It's a straight-forward question, answered in a complex way. It's designed to make the DB do a lot of work. Here's how it works:

```
Month(Date) = Month
{FIXED : MAX(MONTH)} = Max Month
if Month = Max Month then countd(Metric) else null end
```

We do this a varying levels of date granularity. At scale, it's a very hard query to solve. It's also a calculated way to do period-over-period calculations without using a Table Calc.

## The Technology

The list Krinsky and I came up with is quite large (more on that in a minute), but we did some internal vetting to find what might work best for us.  Here's what we are testing

![Snowflake](https://cmtoomey.github.io/img/snowflake.png)

## Snowflake

![Redshift](https://cmtoomey.github.io/img/redshift.png)

## Redshift

![Bq](https://cmtoomey.github.io/img/bigquery.png)

## Big Query

![Jethro](https://cmtoomey.github.io/img/jethro.jpg)

## Jethro

![Presto](https://cmtoomey.github.io/img/presto.png)

## Presto

![Spark SQL](https://cmtoomey.github.io/img/spark.png)

## Spark

![memsql](https://cmtoomey.github.io/img/memsql.png)

## MemSQL

---

# results


<div class='tableauPlaceholder' id='viz1470933626490' style='position: relative'>
    <noscript>
        <a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ta&#47;TableauDragRaceResults&#47;VendorLoadPerformance&#47;1_rss.png' style='border: none' /></a>
    </noscript>
    <object class='tableauViz' style='display:none;'>
        <param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' />
        <param name='site_root' value='' />
        <param name='name' value='TableauDragRaceResults&#47;VendorLoadPerformance' />
        <param name='tabs' value='yes' />
        <param name='toolbar' value='yes' />
        <param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ta&#47;TableauDragRaceResults&#47;VendorLoadPerformance&#47;1.png' />
        <param name='animate_transition' value='yes' />
        <param name='display_static_image' value='yes' />
        <param name='display_spinner' value='yes' />
        <param name='display_overlay' value='yes' />
        <param name='display_count' value='yes' />
    </object>
</div>
<script type='text/javascript'>
    var divElement = document.getElementById('viz1470933626490');
    var vizElement = divElement.getElementsByTagName('object')[0];
    vizElement.style.width = '904px';
    vizElement.style.height = '795px';
    var scriptElement = document.createElement('script');
    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';
    vizElement.parentNode.insertBefore(scriptElement, vizElement);
</script>

As more results come in, I'll update the blog with my initial impressions on each vendor - what they do well and not so well. 


## The future

One of Zillow's core values is **Turn on the Lights**. Just like Tableau, we want data to be available to everyone. Once the project is complete, we are planning on open-sourcing the benchmark, publishing the full list of vendors, and creating a space for others to run the test and upload their results.

We want everyone to know what the state-of-the-art actually is so they can make the best decision for themselves. No BS, no marketing speak, just data.
